{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Activity - AdaBoost\n",
    "## Nick Bias\n",
    "### 4/22/22\n",
    "## Goal: Predict Penguin Species\n",
    "### Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For Gradient Boost\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# For AdaBoosting Models\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# For Comparision Models\n",
    "# compare standalone models for binary classification\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# For splitting data into training and testing \n",
    "from sklearn.model_selection import train_test_split\n",
    "# example of calculate the mean absolute error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# So results are same when re-run\n",
    "import random\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>Biscoe</th>\n",
       "      <th>Dream</th>\n",
       "      <th>Torgersen</th>\n",
       "      <th>MALE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>39.3</td>\n",
       "      <td>20.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>47.2</td>\n",
       "      <td>13.7</td>\n",
       "      <td>214.0</td>\n",
       "      <td>4925.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>46.8</td>\n",
       "      <td>14.3</td>\n",
       "      <td>215.0</td>\n",
       "      <td>4850.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>50.4</td>\n",
       "      <td>15.7</td>\n",
       "      <td>222.0</td>\n",
       "      <td>5750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>45.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>212.0</td>\n",
       "      <td>5200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>Gentoo</td>\n",
       "      <td>49.9</td>\n",
       "      <td>16.1</td>\n",
       "      <td>213.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    species  culmen_length_mm  culmen_depth_mm  flipper_length_mm  \\\n",
       "0    Adelie              39.1             18.7              181.0   \n",
       "1    Adelie              39.5             17.4              186.0   \n",
       "2    Adelie              40.3             18.0              195.0   \n",
       "4    Adelie              36.7             19.3              193.0   \n",
       "5    Adelie              39.3             20.6              190.0   \n",
       "..      ...               ...              ...                ...   \n",
       "338  Gentoo              47.2             13.7              214.0   \n",
       "340  Gentoo              46.8             14.3              215.0   \n",
       "341  Gentoo              50.4             15.7              222.0   \n",
       "342  Gentoo              45.2             14.8              212.0   \n",
       "343  Gentoo              49.9             16.1              213.0   \n",
       "\n",
       "     body_mass_g  Biscoe  Dream  Torgersen  MALE  \n",
       "0         3750.0       0      0          1     1  \n",
       "1         3800.0       0      0          1     0  \n",
       "2         3250.0       0      0          1     0  \n",
       "4         3450.0       0      0          1     0  \n",
       "5         3650.0       0      0          1     1  \n",
       "..           ...     ...    ...        ...   ...  \n",
       "338       4925.0       1      0          0     0  \n",
       "340       4850.0       1      0          0     0  \n",
       "341       5750.0       1      0          0     1  \n",
       "342       5200.0       1      0          0     0  \n",
       "343       5400.0       1      0          0     1  \n",
       "\n",
       "[333 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = pd.read_csv(\"Data/Week4/penguins_size.csv\")\n",
    "\n",
    "# Drop Rows with NA values \n",
    "clean = size.dropna()\n",
    "\n",
    "# Drops row with a '.' for the Sex Variable\n",
    "clean = clean[clean['sex'] != '.']\n",
    "# only 11 rows were dropped \n",
    "\n",
    "# Creating Dummy Variables for Island and Sex\n",
    "island = pd.get_dummies(clean['island'])\n",
    "sex = pd.get_dummies(clean['sex'])\n",
    "\n",
    "# Merging with Original Data\n",
    "penguins = pd.merge(clean, island, left_index=True, right_index=True)\n",
    "penguins = pd.merge(penguins, sex, left_index=True, right_index=True)\n",
    "\n",
    "# Dropping Columns that the Dummies were made from \n",
    "penguins = penguins.drop(['sex', 'island', 'FEMALE'], axis = 1)\n",
    "penguins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Prediction Variable from dataset\n",
    "- X = Dataset with all Independent Variables \n",
    "- y = The Dependent Variable of Penguin Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only Takes Dependent Variable we are tryng to predict \n",
    "y = penguins['species']\n",
    "\n",
    "# Change levels to numeric for XGBoosting \n",
    "y = y.replace('Adelie', 0)\n",
    "y = y.replace('Chinstrap', 1)\n",
    "y = y.replace('Gentoo', 2)\n",
    "# 'Adelie' 'Chinstrap' 'Gentoo'\n",
    "\n",
    "# Takes independent variables that will be used to predict \n",
    "X = penguins.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models to Compare with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a stacking ensemble of models\n",
    "def get_stacking():\n",
    "    # define the base models\n",
    "    level0 = list()\n",
    "    level0.append(('lr', BaggingClassifier(base_estimator=LogisticRegression())))\n",
    "    level0.append(('knn', BaggingClassifier(base_estimator=KNeighborsClassifier())))\n",
    "    level0.append(('cart', BaggingClassifier(base_estimator=DecisionTreeClassifier())))\n",
    "    level0.append(('forest', BaggingClassifier(base_estimator=RandomForestClassifier())))\n",
    "    level0.append(('svm', BaggingClassifier(base_estimator=SVC())))\n",
    "    level0.append(('bayes', BaggingClassifier(base_estimator=GaussianNB())))\n",
    "    level0.append(('adaboost', BaggingClassifier(base_estimator=AdaBoostClassifier())))\n",
    "    level0.append(('gradboost', BaggingClassifier(base_estimator=GradientBoostingClassifier())))\n",
    "    level0.append(('xgboost', BaggingClassifier(base_estimator=XGBClassifier())))\n",
    "    # define meta learner model\n",
    "    level1 = DecisionTreeClassifier()\n",
    "    # define the stacking ensemble\n",
    "    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5, passthrough = True)\n",
    "    return model\n",
    "# passthrough is used for the stacking to take the og dataset instead of just the other model results\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    models['lr'] = LogisticRegression()\n",
    "    models['knn'] = KNeighborsClassifier()\n",
    "    models['cart'] = DecisionTreeClassifier()\n",
    "    models['forest'] = RandomForestClassifier()\n",
    "    models['svm'] = SVC()\n",
    "    models['bayes'] = GaussianNB()\n",
    "    models['adaboost'] = AdaBoostClassifier()\n",
    "    models['gradboost'] = GradientBoostingClassifier()\n",
    "    models['xgboost'] = XGBClassifier()\n",
    "    models['stacking'] = get_stacking()\n",
    "    return models\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores\n",
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">lr 0.988 (0.017)\n",
      ">knn 0.792 (0.043)\n",
      ">cart 0.980 (0.024)\n",
      ">forest 0.994 (0.014)\n",
      ">svm 0.724 (0.038)\n",
      ">bayes 0.841 (0.059)\n",
      ">adaboost 0.813 (0.067)\n",
      ">gradboost 0.986 (0.017)\n",
      ">xgboost 0.988 (0.017)\n",
      ">stacking 0.989 (0.017)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base models of Gradient Boosting and XGBoosting performed much better than a base AdaBoost model. In fact they achieved accuracies of about 17% higher than AdaBoost. Gradient Boosting and XGBoosting perfromed the best, just behind the random forest and logistic regression models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Dataset into a Training and Testing set \n",
    "x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Accuracy: 98.51%\n",
      "Model 2 Accuracy: 49.25%\n",
      "Model 3 Accuracy: 100.00%\n",
      "Model 4 Accuracy: 34.33%\n",
      "Model 5 Accuracy: 49.25%\n",
      "Model 6 Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Default Model\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Base Model Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Model 2 - Low parameters \n",
    "model2 = GradientBoostingClassifier(n_estimators=10, \n",
    "                                    learning_rate=0.01, \n",
    "                                    subsample=0.01, \n",
    "                                    max_depth=0.01, \n",
    "                                    min_impurity_decrease=0.5, \n",
    "                                    random_state=0)\n",
    "model2.fit(x_train, y_train)\n",
    "y_pred2 = model2.predict(x_test)\n",
    "predictions2 = [round(value) for value in y_pred2]\n",
    "accuracy2 = accuracy_score(y_test, predictions2)\n",
    "print(\"Model 2 Accuracy: %.2f%%\" % (accuracy2 * 100.0))\n",
    "\n",
    "# Model 3 - High parameters \n",
    "model3 = GradientBoostingClassifier(n_estimators=1000, \n",
    "                                    learning_rate=1, \n",
    "                                    subsample=1, \n",
    "                                    max_depth=10, \n",
    "                                    min_impurity_decrease=1, \n",
    "                                    random_state=0)\n",
    "model3.fit(x_train, y_train)\n",
    "y_pred3 = model3.predict(x_test)\n",
    "predictions3 = [round(value) for value in y_pred3]\n",
    "accuracy3 = accuracy_score(y_test, predictions3)\n",
    "print(\"Model 3 Accuracy: %.2f%%\" % (accuracy3 * 100.0))\n",
    "\n",
    "# Model 4 - Low subsample \n",
    "model4 = GradientBoostingClassifier(n_estimators=1000, \n",
    "                                    learning_rate=1, \n",
    "                                    subsample=0.01, \n",
    "                                    max_depth=10, \n",
    "                                    min_impurity_decrease=1, \n",
    "                                    random_state=0)\n",
    "model4.fit(x_train, y_train)\n",
    "y_pred4 = model4.predict(x_test)\n",
    "predictions4 = [round(value) for value in y_pred4]\n",
    "accuracy4 = accuracy_score(y_test, predictions4)\n",
    "print(\"Model 4 Accuracy: %.2f%%\" % (accuracy4 * 100.0))\n",
    "\n",
    "# Model 5 - Low Max_depth\n",
    "model5 = GradientBoostingClassifier(n_estimators=1000, \n",
    "                                    learning_rate=1, \n",
    "                                    subsample=1, \n",
    "                                    max_depth=0.5, \n",
    "                                    min_impurity_decrease=1, \n",
    "                                    random_state=0)\n",
    "model5.fit(x_train, y_train)\n",
    "y_pred5 = model5.predict(x_test)\n",
    "predictions5 = [round(value) for value in y_pred5]\n",
    "accuracy5 = accuracy_score(y_test, predictions5)\n",
    "print(\"Model 5 Accuracy: %.2f%%\" % (accuracy5 * 100.0))\n",
    "\n",
    "# Model 6 - high min_impurity_decrease\n",
    "model6 = GradientBoostingClassifier(n_estimators=1000, \n",
    "                                    learning_rate=1, \n",
    "                                    subsample=1, \n",
    "                                    max_depth=5, \n",
    "                                    min_impurity_decrease=1, \n",
    "                                    random_state=0)\n",
    "model6.fit(x_train, y_train)\n",
    "y_pred6 = model6.predict(x_test)\n",
    "predictions6 = [round(value) for value in y_pred6]\n",
    "accuracy6 = accuracy_score(y_test, predictions6)\n",
    "print(\"Model 6 Accuracy: %.2f%%\" % (accuracy6 * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Base Gradient Boost Model achieved an accuracy of 95.5%. This will be what the other models are compared to. \n",
    "- Model 2 was given low parameters. It achieved almost the lowest accuracy score because of it. The low learning rate means that there is decreased contribution of each classifier. Low n-estimators means that less weak learners are used in training.  \n",
    "- Model 3 was given high parameters. It acchieved the highest accuracy out of all the models. It is basically the opposite of Model 2 \n",
    "- Model 4 was given just a low subsample. A subsample smaller than 1.0 this results in Stochastic Gradient Boosting. This achieved the lowest accuracy score. \n",
    "- Model 5 was given a low maximum depth. A small maximum depth limits the number of nodes in the tree. It achieved almost the lowest accuracy score.\n",
    "- Model 6 was given a low minimum impurity decrease. This means a node will be split if this split induces a decrease of the impurity greater than or equal to this value. This model achieved one of the best accuracy scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Accuracy: 98.51%\n",
      "Model 2 Accuracy: 98.51%\n",
      "Model 3 Accuracy: 82.09%\n",
      "Model 4 Accuracy: 98.51%\n",
      "Model 5  Accuracy: 83.58%\n",
      "Model 6  Accuracy: 49.25%\n"
     ]
    }
   ],
   "source": [
    "# Default Model \n",
    "model = XGBClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Base Model Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Model 2 - linear booster\n",
    "model2 = XGBClassifier(booster = 'gblinear')\n",
    "model2.fit(x_train, y_train)\n",
    "y_pred2 = model2.predict(x_test)\n",
    "predictions2 = [round(value) for value in y_pred2]\n",
    "accuracy2 = accuracy_score(y_test, predictions2)\n",
    "print(\"Model 2 Accuracy: %.2f%%\" % (accuracy2 * 100.0))\n",
    "\n",
    "# Model 3 - High Parameters \n",
    "model3 = XGBClassifier(n_estimators=1000, \n",
    "                       learning_rate = 1, \n",
    "                       gamma = 100, \n",
    "                       max_depth = 10, \n",
    "                       min_child_weight = 10, \n",
    "                       max_delta_step = 10, \n",
    "                       subsample = 1,\n",
    "                       reg_lambda = 10,\n",
    "                       alpha = 10)\n",
    "model3.fit(x_train, y_train)\n",
    "y_pred3 = model3.predict(x_test)\n",
    "predictions3 = [round(value) for value in y_pred3]\n",
    "accuracy3 = accuracy_score(y_test, predictions3)\n",
    "print(\"Model 3 Accuracy: %.2f%%\" % (accuracy3 * 100.0))\n",
    "\n",
    "# Model 4 - Low Parameters \n",
    "model4 = XGBClassifier(n_estimators=10, \n",
    "                       learning_rate = 0.01, \n",
    "                       gamma = 1, \n",
    "                       max_depth = 1, \n",
    "                       min_child_weight = 0.1, \n",
    "                       max_delta_step = 0.1, \n",
    "                       subsample = 0.1,\n",
    "                       reg_lambda = 1,\n",
    "                       alpha = 1)\n",
    "model4.fit(x_train, y_train)\n",
    "y_pred4 = model4.predict(x_test)\n",
    "predictions4 = [round(value) for value in y_pred4]\n",
    "accuracy4 = accuracy_score(y_test, predictions4)\n",
    "print(\"Model 4 Accuracy: %.2f%%\" % (accuracy4 * 100.0))\n",
    "\n",
    "# Model 5 - High Gamma\n",
    "model5 = XGBClassifier(gamma = 100)\n",
    "model5.fit(x_train, y_train)\n",
    "y_pred5 = model5.predict(x_test)\n",
    "predictions5 = [round(value) for value in y_pred5]\n",
    "accuracy5 = accuracy_score(y_test, predictions5)\n",
    "print(\"Model 5  Accuracy: %.2f%%\" % (accuracy5 * 100.0))\n",
    "\n",
    "# Model 5 - High alpha\n",
    "model6 = XGBClassifier(alpha = 100)\n",
    "model6.fit(x_train, y_train)\n",
    "y_pred6 = model6.predict(x_test)\n",
    "predictions6 = [round(value) for value in y_pred6]\n",
    "accuracy6 = accuracy_score(y_test, predictions6)\n",
    "print(\"Model 6  Accuracy: %.2f%%\" % (accuracy6 * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The XGBoosted base model with default parameters achieved an accuracy of 97%. This will be what the other models are compared to.\n",
    "- Model 2 had a linear function booster. This achieved the highest accuracy out of all the models. \n",
    "- Model 3 was given high parameters. It had a low accuracy. The high min_child  wieght is the Minimum sum of instance weight (hessian) needed in a child node. Max_delta_step is Maximum delta step we allow each leaf output to be. If the value is set to 0, it means there is no constraint.\n",
    "- Model 4 was given low parameters and achieved a higher accuracy score than Model 3\n",
    "- Model 5 was given a high Gamma. This resulted in a bad accuracy score. Gamma is the minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "- Model 6 was given a high alpha. This had the worst accuracy. Alpha is the L1 regularization term on weights. Increasing this value will make model more conservative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared with AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the AdaBoost PA, these accuracy scores varied much more. It seems that small changes in any variable can have drastic effects on the performance of the models. Unlike AdaBoosting, where you had to change many variables to get a different accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
